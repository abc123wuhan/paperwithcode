{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow import keras\n",
    "import keras as K\n",
    "from keras.layers.core import Dense\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.losses import mean_squared_error\n",
    "from keras import backend\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import time\n",
    "start=time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainenable = 1\n",
    "data = pd.read_csv(\"/project/env_lstm/Swarm_data.csv\", header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens = data[8]                                                            \n",
    "dens = dens.astype(\"float\")                                             \n",
    "df_feature = data.drop([ 0, 1, 2, 8, 14, 15, 16], axis=1)                         \n",
    "df_feature = df_feature.astype(\"float\")                                 \n",
    "df_feature = df_feature.loc[np.isfinite(df_feature).all(axis=1).squeeze(),:]\n",
    "dens = dens[np.isfinite(df_feature).all(axis=1).squeeze()]\n",
    "df_feature = df_feature.loc[np.isfinite(dens).squeeze(),:]\n",
    "dens = dens[np.isfinite(dens).squeeze()]\n",
    "df_feature = df_feature.reset_index(drop=True)\n",
    "df_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array(df_feature.index)                                           \n",
    "n_data = len(df_feature)                                                   \n",
    "train_ind = np.random.permutation(idx[:int(n_data * .8)])                   \n",
    "valid_ind = np.random.permutation(idx[int(n_data * .8):int(n_data * .9)])    \n",
    "test_ind = idx[int(n_data * .9):int(n_data * 1)]\n",
    "len(test_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind_ = idx[:int(n_data * .8)]                  \n",
    "valid_ind_ = idx[int(n_data * .8):int(n_data * .9)]    \n",
    "test_ind_ = idx[int(n_data * .9):int(n_data * 1)]\n",
    "a=data[0]\n",
    "print('训练数据：',a[train_ind_[0]],'-',a[train_ind_[-1]])\n",
    "print('验证数据：',a[valid_ind_[0]],'-',a[valid_ind_[-1]])\n",
    "print('测试数据：',a[test_ind_[0]],'-',a[test_ind_[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = \"2013-03-17\"  \n",
    "end_time =  \"2013-03-19\"     \n",
    "test_ind = data[(data[0]>=start_time) & (data[0]<=end_time)].index\n",
    "print(test_ind[0],test_ind[-1],test_ind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_data = StandardScaler()             \n",
    "scaler_dens = StandardScaler()             \n",
    "train_feature = df_feature.loc[train_ind]  \n",
    "scaler_data.fit(train_feature)               \n",
    "feature = scaler_data.transform(df_feature)  \n",
    "scaler_dens.fit(dens.values.reshape(-1, 1))   \n",
    "dens = scaler_dens.transform(dens.values.reshape(-1, 1))   \n",
    "feature = np.array(feature)\n",
    "dens = np.reshape(dens, (-1, 1))\n",
    "\n",
    "history_length = 5     \n",
    "swarm_nums = 8        \n",
    "omni2_num = 54        \n",
    "ensembel = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(input_dim = 224):  \n",
    "#     import os\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "    model = K.models.Sequential()\n",
    "\n",
    "    model.add(K.layers.Dense(units=64, input_dim=input_dim, kernel_initializer = 'normal', activation='sigmoid')) \n",
    "    model.add(K.layers.Dropout(0.2))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(units=32, kernel_initializer = 'normal', activation='sigmoid'))\n",
    "    model.add(K.layers.Dropout(0.2))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(units=16, kernel_initializer = 'normal', activation='sigmoid'))\n",
    "    model.add(K.layers.Dropout(0.2))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(units=16, kernel_initializer = 'normal', activation='sigmoid'))\n",
    "    model.add(K.layers.Dropout(0.2))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(units=1))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss=rmse, optimizer='Adamax' ,metrics=['mse'])   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arrays_from_file(train_index):    \n",
    "    while 1:    \n",
    "        for j in range(batch_step_nums):             \n",
    "            cur_idx = train_index[j*batch_size: (j+1)*batch_size] \n",
    "            cur_feature = np.concatenate([feature[cur_idx - k, 8:] for k in range(1, history_length)], axis=-1) \n",
    "            cur_feature = np.concatenate([cur_feature, feature[cur_idx, :8]], axis=-1)\n",
    "            cur_dens = dens[cur_idx]              \n",
    "            yield (cur_feature, cur_dens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(train_feature) // 10 \n",
    "parts = []\n",
    "for i in range(10):\n",
    "    parts.append(train_feature.index[i * length: length + i * length])\n",
    "    if i == 9:\n",
    "        parts[-1] = train_feature.index[i * length: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(10, shuffle=False, random_state=None)\n",
    "A = []\n",
    "B = []\n",
    "Q = []\n",
    "\n",
    "for train_idx in parts:\n",
    "    train_index, test_index = train_idx[:int(len(train_idx) * 0.9)], train_idx[int(len(train_idx) * 0.9):]\n",
    "    Q.append(test_index)\n",
    "\n",
    "    def rmse(y_true, y_pre):\n",
    "        return backend.sqrt(mean_squared_error(y_true, y_pre))\n",
    "    \n",
    "    model = models()\n",
    "    model.compile(loss=rmse,optimizer='Adamax', metrics=['mse'])  \n",
    "\n",
    "    filepath = f'model_ensembel_{ensembel}.h5' \n",
    "    checkpoint = ModelCheckpoint(filepath,  monitor='val_mse', verbose=0, save_best_only=True, mode='min', period=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    batch_size = 128\n",
    "    batch_step_nums = len(train_index) // batch_size\n",
    "\n",
    "    valid_ind = np.random.permutation(idx[int(n_data * .8):int(n_data * .9)])\n",
    "    valid_feature = np.concatenate([feature[valid_ind - k, 8:] for k in range(1, history_length)], axis=-1)\n",
    "    valid_feature = np.concatenate([valid_feature, feature[valid_ind, :8]], axis=-1) \n",
    "    valid_dens = dens[valid_ind]\n",
    "    if trainenable:\n",
    "        history=model.fit_generator(generate_arrays_from_file(train_index),\n",
    "                            steps_per_epoch=batch_step_nums,\n",
    "                            validation_data=(valid_feature, valid_dens),\n",
    "                            epochs=20,\n",
    "                            max_queue_size=1,\n",
    "                            workers=1,\n",
    "                            callbacks=callbacks_list)\n",
    "    test_feature = np.concatenate([feature[test_index - k, 8:] for k in range(1, history_length)], axis=-1)\n",
    "\n",
    "    test_feature = np.concatenate([test_feature, feature[test_index, :8]], axis=-1) \n",
    "    test_dens = dens[test_index]   \n",
    "    model = load_model(f'model_ensembel_{ensembel}.h5',custom_objects={'rmse': rmse}) \n",
    "    pr = model.predict(test_feature)\n",
    "    A.append(pr)\n",
    "    test_feature = np.concatenate([feature[test_ind - k, 8:] for k in range(1, history_length)], axis=-1)\n",
    "    test_feature = np.concatenate([test_feature, feature[test_ind, :8]], axis=-1)\n",
    "    pr = model.predict(test_feature)\n",
    "    B.append(pr)\n",
    "\n",
    "    ensembel += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC = np.vstack(A)\n",
    "BC = np.hstack(B)\n",
    "BC = BC.mean(axis=1)\n",
    "QI = np.hstack(Q)\n",
    "print(AC.shape)\n",
    "print(BC.shape)\n",
    "print(QI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1])),\n",
    "                                 tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10)),\n",
    "                                 keras.layers.Dense(1)])\n",
    "\n",
    "def rmse(y_true, y_pre):\n",
    "    return backend.sqrt(mean_squared_error(y_true, y_pre))\n",
    "\n",
    "model.compile(loss=rmse,\n",
    "              optimizer='Adamax', metrics=['mse'])\n",
    "\n",
    "filepath = 'model_ensembel_final.h5'\n",
    "checkpoint = ModelCheckpoint(filepath,  monitor='val_mse', verbose=1, save_best_only=True, mode='min', period=1)\n",
    "callbacks_list = [checkpoint]\n",
    "if trainenable:\n",
    "    history1=model.fit(AC.reshape(-1, 1, 1),dens[QI],\n",
    "              epochs=20,\n",
    "              batch_size = 128,\n",
    "              verbose=1,\n",
    "              validation_split = 0.1,\n",
    "              callbacks=callbacks_list)\n",
    "    # model.fit():将训练数据在模型中训练一定次数，返回loss和测量指标\n",
    "end=time.clock()\n",
    "use_time=end-start\n",
    "print('train time:',use_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(6, 4) # 图像尺寸大小\n",
    "loss1=history1.history[\"loss\"]\n",
    "test_loss1 = history1.history[\"val_loss\"]\n",
    "plt.plot(loss1, label='train', color='r')\n",
    "plt.plot(test_loss1, label='test',color='b')\n",
    "x_major_locator=MultipleLocator(2) \n",
    "ax=plt.gca()\n",
    "ax.xaxis.set_major_locator(x_major_locator)\n",
    "plt.xlim(-0.5,20)\n",
    "plt.yticks(fontproperties='Times New Roman', size=10)\n",
    "plt.xticks(fontproperties='Times New Roman', size=10)\n",
    "plt.xlabel('epoch', fontsize=15, labelpad=1)\n",
    "plt.ylabel('loss', fontsize=15, labelpad=1)\n",
    "plt.tick_params(direction='in', width=1, length=2, pad=3)\n",
    "plt.legend(fontsize=10)\n",
    "plt.savefig(\"loss_epoch.jpg\",bbox_inches='tight',dpi=300,pad_inches=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# densitypre = scaler_dens.inverse_transform(AC.reshape(-1,1))\n",
    "# densityopt = scaler_dens.inverse_transform(dens[QI].reshape(-1,1))\n",
    "densitypre = scaler_dens.inverse_transform(BC.reshape(-1,1))\n",
    "densityopt = scaler_dens.inverse_transform(dens[test_ind].reshape(-1,1))\n",
    "print(densitypre[0:1],'\\n',densityopt[:1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "mse = np.sum(( densityopt - (densitypre)) ** 2) / len(densityopt)\n",
    "rmse = sqrt(mse)\n",
    "r2 = 1-mse/ np.var(densityopt)\n",
    "print('rmse',rmse,'r2',r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
